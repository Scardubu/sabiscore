#!/usr/bin/env python3
"""
Production ML Model for 90%+ Accuracy
=====================================

Key Design Principles:
1. Market Odds = Best Signal (Pinnacle CLV is gold standard)
2. Form Regression = Secondary Signal
3. Ensemble Diversity = Better Generalization
4. Proper Calibration = Accurate Probabilities

Model Architecture:
- Level 1: Diverse base learners (XGB, LGB, CatBoost, RF)
- Level 2: Calibrated probability outputs
- Level 3: Meta-learner combining calibrated outputs
- Level 4: Ensemble with market odds override

Important Reality Check:
- Sports prediction 90%+ accuracy is unrealistic
- Professional sharp bettors achieve 53-58% long-term
- Market-beating requires ~52.4% at standard odds
- Our goal: Maximize edge, not achieve impossible accuracy
"""

import logging
import numpy as np
import pandas as pd
import joblib
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from datetime import datetime
import json
import warnings
warnings.filterwarnings('ignore')

# ML imports
from sklearn.model_selection import (
    TimeSeriesSplit, 
    cross_val_score,
    cross_val_predict
)
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression, RidgeClassifier
from sklearn.ensemble import (
    RandomForestClassifier, 
    GradientBoostingClassifier,
    ExtraTreesClassifier,
    VotingClassifier,
    StackingClassifier
)
from sklearn.calibration import CalibratedClassifierCV
from sklearn.metrics import (
    accuracy_score, 
    log_loss,
    brier_score_loss,
    classification_report,
    confusion_matrix
)

# Gradient boosting libraries
try:
    import xgboost as xgb
    HAS_XGB = True
except ImportError:
    HAS_XGB = False
    
try:
    import lightgbm as lgb
    HAS_LGB = True
except ImportError:
    HAS_LGB = False
    
try:
    from catboost import CatBoostClassifier
    HAS_CATBOOST = True
except ImportError:
    HAS_CATBOOST = False

logger = logging.getLogger(__name__)

# Paths
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
MODEL_DIR = PROJECT_ROOT / "models"
DATA_DIR = PROJECT_ROOT / "data" / "processed"

MODEL_DIR.mkdir(parents=True, exist_ok=True)


class ProductionMLModel:
    """
    Production-grade ML model for match outcome prediction.
    
    Features:
    - Stacked ensemble with diverse base learners
    - Probability calibration for accurate confidence
    - Time-series cross-validation (no future leakage)
    - Market odds integration
    - Feature importance analysis
    """
    
    def __init__(
        self,
        model_name: str = "sabiscore_v2",
        n_folds: int = 5,
        random_state: int = 42
    ):
        self.model_name = model_name
        self.n_folds = n_folds
        self.random_state = random_state
        
        self.scaler = StandardScaler()
        self.model = None
        self.feature_names: List[str] = []
        self.feature_importances: Dict[str, float] = {}
        self.training_metrics: Dict[str, Any] = {}
        
        self._build_ensemble()
        
    def _build_ensemble(self):
        """Build diverse ensemble architecture"""
        
        base_estimators = []
        
        # XGBoost - gradient boosting champion
        if HAS_XGB:
            xgb_model = xgb.XGBClassifier(
                n_estimators=500,
                max_depth=6,
                learning_rate=0.05,
                subsample=0.8,
                colsample_bytree=0.8,
                reg_alpha=0.1,
                reg_lambda=1.0,
                min_child_weight=3,
                objective='multi:softprob',
                num_class=3,
                random_state=self.random_state,
                n_jobs=-1,
                use_label_encoder=False,
                eval_metric='mlogloss'
            )
            base_estimators.append(('xgb', xgb_model))
            logger.info("âœ… Added XGBoost to ensemble")
            
        # LightGBM - fast and accurate
        if HAS_LGB:
            lgb_model = lgb.LGBMClassifier(
                n_estimators=500,
                max_depth=6,
                learning_rate=0.05,
                num_leaves=40,
                subsample=0.8,
                colsample_bytree=0.8,
                reg_alpha=0.1,
                reg_lambda=1.0,
                min_child_samples=20,
                objective='multiclass',
                num_class=3,
                random_state=self.random_state,
                n_jobs=-1,
                verbose=-1
            )
            base_estimators.append(('lgb', lgb_model))
            logger.info("âœ… Added LightGBM to ensemble")
            
        # CatBoost - handles categorical well
        if HAS_CATBOOST:
            cat_model = CatBoostClassifier(
                iterations=500,
                depth=6,
                learning_rate=0.05,
                l2_leaf_reg=3.0,
                random_state=self.random_state,
                verbose=0,
                thread_count=-1
            )
            base_estimators.append(('catboost', cat_model))
            logger.info("âœ… Added CatBoost to ensemble")
            
        # Random Forest - different approach
        rf_model = RandomForestClassifier(
            n_estimators=300,
            max_depth=10,
            min_samples_split=10,
            min_samples_leaf=5,
            max_features='sqrt',
            random_state=self.random_state,
            n_jobs=-1,
            class_weight='balanced'
        )
        base_estimators.append(('rf', rf_model))
        logger.info("âœ… Added Random Forest to ensemble")
        
        # Extra Trees - more randomization
        et_model = ExtraTreesClassifier(
            n_estimators=300,
            max_depth=10,
            min_samples_split=10,
            min_samples_leaf=5,
            max_features='sqrt',
            random_state=self.random_state,
            n_jobs=-1,
            class_weight='balanced'
        )
        base_estimators.append(('et', et_model))
        logger.info("âœ… Added Extra Trees to ensemble")
        
        # Gradient Boosting (sklearn) - reliable fallback
        gb_model = GradientBoostingClassifier(
            n_estimators=200,
            max_depth=5,
            learning_rate=0.1,
            subsample=0.8,
            min_samples_split=10,
            random_state=self.random_state
        )
        base_estimators.append(('gb', gb_model))
        logger.info("âœ… Added Gradient Boosting to ensemble")
        
        # Meta-learner for stacking
        meta_learner = LogisticRegression(
            C=1.0,
            max_iter=1000,
            random_state=self.random_state,
            multi_class='multinomial',
            solver='lbfgs'
        )
        
        # Build stacking ensemble
        self.model = StackingClassifier(
            estimators=base_estimators,
            final_estimator=meta_learner,
            cv=5,
            stack_method='predict_proba',
            n_jobs=-1,
            passthrough=True  # Include original features for meta-learner
        )
        
        logger.info(f"âœ… Built ensemble with {len(base_estimators)} base learners")
        
    def train(
        self,
        X: pd.DataFrame,
        y: pd.Series,
        validate: bool = True
    ) -> Dict[str, Any]:
        """
        Train the model with proper time-series validation.
        
        Args:
            X: Feature matrix
            y: Target vector (0=Away, 1=Draw, 2=Home)
            validate: Whether to run cross-validation
            
        Returns:
            Training metrics dictionary
        """
        logger.info("="*60)
        logger.info("TRAINING PRODUCTION ML MODEL")
        logger.info("="*60)
        
        self.feature_names = list(X.columns)
        
        # Scale features
        X_scaled = self.scaler.fit_transform(X)
        X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)
        
        # Validate with time-series split (no future leakage)
        if validate:
            metrics = self._validate_model(X_scaled, y)
        else:
            metrics = {}
            
        # Train final model on all data
        logger.info("ðŸš€ Training final model on all data...")
        start_time = datetime.now()
        
        self.model.fit(X_scaled, y)
        
        training_time = (datetime.now() - start_time).total_seconds()
        logger.info(f"âœ… Training completed in {training_time:.1f}s")
        
        # Calculate feature importances
        self._calculate_feature_importance(X_scaled, y)
        
        # Store metrics
        self.training_metrics = {
            **metrics,
            'training_time_seconds': training_time,
            'n_samples': len(X),
            'n_features': len(self.feature_names),
            'training_date': datetime.now().isoformat()
        }
        
        return self.training_metrics
        
    def _validate_model(
        self,
        X: pd.DataFrame,
        y: pd.Series
    ) -> Dict[str, float]:
        """
        Validate model with time-series cross-validation.
        
        Uses TimeSeriesSplit to ensure no future data leakage.
        """
        logger.info("ðŸ“Š Running time-series cross-validation...")
        
        # Time series split
        tscv = TimeSeriesSplit(n_splits=self.n_folds)
        
        fold_metrics = []
        
        for fold_idx, (train_idx, val_idx) in enumerate(tscv.split(X)):
            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
            
            # Train fold model
            fold_model = self._build_simple_ensemble()
            fold_model.fit(X_train, y_train)
            
            # Predictions
            y_pred = fold_model.predict(X_val)
            y_proba = fold_model.predict_proba(X_val)
            
            # Metrics
            acc = accuracy_score(y_val, y_pred)
            ll = log_loss(y_val, y_proba)
            
            fold_metrics.append({
                'fold': fold_idx + 1,
                'accuracy': acc,
                'log_loss': ll,
                'train_size': len(train_idx),
                'val_size': len(val_idx)
            })
            
            logger.info(f"  Fold {fold_idx + 1}: Accuracy={acc:.4f}, LogLoss={ll:.4f}")
            
        # Aggregate metrics
        avg_acc = np.mean([m['accuracy'] for m in fold_metrics])
        std_acc = np.std([m['accuracy'] for m in fold_metrics])
        avg_ll = np.mean([m['log_loss'] for m in fold_metrics])
        
        logger.info(f"ðŸ“Š CV Results: Accuracy={avg_acc:.4f} (+/-{std_acc:.4f})")
        
        return {
            'cv_accuracy_mean': avg_acc,
            'cv_accuracy_std': std_acc,
            'cv_log_loss_mean': avg_ll,
            'fold_metrics': fold_metrics
        }
        
    def _build_simple_ensemble(self):
        """Build simpler ensemble for CV (faster)"""
        estimators = []
        
        if HAS_XGB:
            estimators.append(('xgb', xgb.XGBClassifier(
                n_estimators=200, max_depth=5, learning_rate=0.1,
                random_state=self.random_state, use_label_encoder=False,
                eval_metric='mlogloss', n_jobs=-1
            )))
        
        if HAS_LGB:
            estimators.append(('lgb', lgb.LGBMClassifier(
                n_estimators=200, max_depth=5, learning_rate=0.1,
                random_state=self.random_state, verbose=-1, n_jobs=-1
            )))
            
        estimators.append(('rf', RandomForestClassifier(
            n_estimators=100, max_depth=8,
            random_state=self.random_state, n_jobs=-1
        )))
        
        return VotingClassifier(estimators, voting='soft', n_jobs=-1)
        
    def _calculate_feature_importance(
        self,
        X: pd.DataFrame,
        y: pd.Series
    ):
        """Calculate feature importances from base models"""
        
        importances = {}
        
        # Try to extract from each base model
        for name, model in self.model.named_estimators_.items():
            if hasattr(model, 'feature_importances_'):
                for i, imp in enumerate(model.feature_importances_):
                    feat_name = self.feature_names[i]
                    if feat_name not in importances:
                        importances[feat_name] = []
                    importances[feat_name].append(imp)
                    
        # Average importances across models
        self.feature_importances = {
            k: np.mean(v) for k, v in importances.items()
        }
        
        # Sort by importance
        self.feature_importances = dict(
            sorted(self.feature_importances.items(), 
                   key=lambda x: x[1], reverse=True)
        )
        
        # Log top features
        logger.info("\nðŸ“Š Top 15 Most Important Features:")
        for i, (feat, imp) in enumerate(list(self.feature_importances.items())[:15]):
            logger.info(f"  {i+1}. {feat}: {imp:.4f}")
            
    def predict(
        self,
        X: pd.DataFrame,
        return_proba: bool = True
    ) -> Dict[str, Any]:
        """
        Make predictions with confidence scores.
        
        Args:
            X: Feature matrix
            return_proba: Whether to return probabilities
            
        Returns:
            Dictionary with predictions and probabilities
        """
        # Ensure columns match training
        X = X[self.feature_names]
        
        # Scale
        X_scaled = self.scaler.transform(X)
        
        # Predict
        predictions = self.model.predict(X_scaled)
        
        result = {
            'predictions': predictions,
            'prediction_labels': ['Away', 'Draw', 'Home']
        }
        
        if return_proba:
            probas = self.model.predict_proba(X_scaled)
            result['probabilities'] = probas
            result['confidence'] = np.max(probas, axis=1)
            
        return result
        
    def predict_match(
        self,
        features: Dict[str, float],
        market_odds: Optional[Dict[str, float]] = None
    ) -> Dict[str, Any]:
        """
        Predict single match with market odds integration.
        
        Args:
            features: Dictionary of feature values
            market_odds: Optional dict with 'home', 'draw', 'away' odds
            
        Returns:
            Prediction with model and market probabilities
        """
        # Create feature vector
        X = pd.DataFrame([features])
        
        # Ensure all features present
        for col in self.feature_names:
            if col not in X.columns:
                X[col] = 0.0
                
        X = X[self.feature_names]
        X_scaled = self.scaler.transform(X)
        
        # Model prediction
        model_proba = self.model.predict_proba(X_scaled)[0]
        model_pred = np.argmax(model_proba)
        
        result = {
            'model_prediction': ['Away', 'Draw', 'Home'][model_pred],
            'model_probabilities': {
                'away': float(model_proba[0]),
                'draw': float(model_proba[1]),
                'home': float(model_proba[2])
            },
            'model_confidence': float(np.max(model_proba))
        }
        
        # Integrate market odds if available
        if market_odds:
            # Convert odds to probabilities (remove vig)
            imp_home = 1 / market_odds['home']
            imp_draw = 1 / market_odds['draw']
            imp_away = 1 / market_odds['away']
            total = imp_home + imp_draw + imp_away
            
            market_proba = {
                'home': imp_home / total,
                'draw': imp_draw / total,
                'away': imp_away / total
            }
            
            result['market_probabilities'] = market_proba
            
            # Blend model and market (market is usually sharper)
            # Use 70% market, 30% model
            blend_weight_model = 0.3
            
            blended_proba = {
                'home': blend_weight_model * model_proba[2] + (1 - blend_weight_model) * market_proba['home'],
                'draw': blend_weight_model * model_proba[1] + (1 - blend_weight_model) * market_proba['draw'],
                'away': blend_weight_model * model_proba[0] + (1 - blend_weight_model) * market_proba['away']
            }
            
            result['blended_probabilities'] = blended_proba
            
            # Find best value (model vs market edge)
            edges = {
                'home': model_proba[2] - market_proba['home'],
                'draw': model_proba[1] - market_proba['draw'],
                'away': model_proba[0] - market_proba['away']
            }
            
            result['value_edges'] = edges
            
            # Identify value bets (model thinks higher probability than market)
            value_threshold = 0.02  # 2% edge
            result['value_bets'] = [
                outcome for outcome, edge in edges.items()
                if edge > value_threshold
            ]
            
        return result
        
    def save(self, path: Optional[Path] = None) -> Path:
        """Save model to disk"""
        if path is None:
            path = MODEL_DIR / f"{self.model_name}.joblib"
            
        save_data = {
            'model': self.model,
            'scaler': self.scaler,
            'feature_names': self.feature_names,
            'feature_importances': self.feature_importances,
            'training_metrics': self.training_metrics,
            'model_name': self.model_name,
            'version': '2.0'
        }
        
        joblib.dump(save_data, path)
        logger.info(f"âœ… Model saved to {path}")
        
        # Also save metadata as JSON
        meta_path = path.with_suffix('.json')
        with open(meta_path, 'w') as f:
            json.dump({
                'feature_names': self.feature_names,
                'feature_importances': self.feature_importances,
                'training_metrics': self.training_metrics,
                'model_name': self.model_name
            }, f, indent=2, default=str)
            
        return path
        
    @classmethod
    def load(cls, path: Optional[Path] = None, model_name: str = "sabiscore_v2") -> 'ProductionMLModel':
        """Load model from disk"""
        if path is None:
            path = MODEL_DIR / f"{model_name}.joblib"
            
        save_data = joblib.load(path)
        
        instance = cls(model_name=save_data['model_name'])
        instance.model = save_data['model']
        instance.scaler = save_data['scaler']
        instance.feature_names = save_data['feature_names']
        instance.feature_importances = save_data.get('feature_importances', {})
        instance.training_metrics = save_data.get('training_metrics', {})
        
        logger.info(f"âœ… Model loaded from {path}")
        return instance


def evaluate_model_realistically(
    model: ProductionMLModel,
    X_test: pd.DataFrame,
    y_test: pd.Series,
    odds_test: Optional[pd.DataFrame] = None
) -> Dict[str, Any]:
    """
    Comprehensive model evaluation with realistic metrics.
    
    Includes:
    - Standard ML metrics (accuracy, log loss)
    - Calibration analysis
    - Betting profitability simulation
    - Comparison vs market baseline
    """
    logger.info("="*60)
    logger.info("MODEL EVALUATION")
    logger.info("="*60)
    
    # Predictions
    result = model.predict(X_test)
    y_pred = result['predictions']
    y_proba = result['probabilities']
    
    # Basic metrics
    accuracy = accuracy_score(y_test, y_pred)
    ll = log_loss(y_test, y_proba)
    
    logger.info(f"Accuracy: {accuracy:.4f}")
    logger.info(f"Log Loss: {ll:.4f}")
    
    # Class-specific metrics
    logger.info("\nClassification Report:")
    logger.info(classification_report(
        y_test, y_pred,
        target_names=['Away', 'Draw', 'Home']
    ))
    
    # Confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    logger.info(f"\nConfusion Matrix:\n{cm}")
    
    # Result distribution
    actual_dist = y_test.value_counts(normalize=True).sort_index()
    pred_dist = pd.Series(y_pred).value_counts(normalize=True).sort_index()
    
    logger.info("\nResult Distribution:")
    logger.info(f"  Actual: Home={actual_dist.get(2, 0):.3f}, Draw={actual_dist.get(1, 0):.3f}, Away={actual_dist.get(0, 0):.3f}")
    logger.info(f"  Predicted: Home={pred_dist.get(2, 0):.3f}, Draw={pred_dist.get(1, 0):.3f}, Away={pred_dist.get(0, 0):.3f}")
    
    evaluation = {
        'accuracy': accuracy,
        'log_loss': ll,
        'confusion_matrix': cm.tolist(),
        'actual_distribution': actual_dist.to_dict(),
        'predicted_distribution': pred_dist.to_dict()
    }
    
    # Betting simulation (if odds available)
    if odds_test is not None and 'pinnacle_home' in odds_test.columns:
        evaluation['betting_analysis'] = _simulate_betting(
            y_test, y_pred, y_proba, odds_test
        )
        
    return evaluation


def _simulate_betting(
    y_true: pd.Series,
    y_pred: np.ndarray,
    y_proba: np.ndarray,
    odds_df: pd.DataFrame,
    stake: float = 1.0
) -> Dict[str, Any]:
    """
    Simulate betting performance.
    
    Key insight: Even 55% accuracy can be profitable with good odds.
    """
    logger.info("\nðŸ’° Betting Simulation:")
    
    # Get odds columns
    home_odds = odds_df['pinnacle_home'].values
    draw_odds = odds_df['pinnacle_draw'].values
    away_odds = odds_df['pinnacle_away'].values
    
    results = {
        'flat_betting': {'profit': 0, 'bets': 0},
        'kelly_betting': {'profit': 0, 'bets': 0},
        'value_betting': {'profit': 0, 'bets': 0}
    }
    
    for i in range(len(y_true)):
        actual = y_true.iloc[i]
        pred = y_pred[i]
        probs = y_proba[i]
        
        # Flat betting on prediction
        if pred == 0:  # Away
            odds = away_odds[i]
        elif pred == 1:  # Draw
            odds = draw_odds[i]
        else:  # Home
            odds = home_odds[i]
            
        if not np.isnan(odds):
            results['flat_betting']['bets'] += 1
            if actual == pred:
                results['flat_betting']['profit'] += (odds - 1) * stake
            else:
                results['flat_betting']['profit'] -= stake
                
        # Value betting (only when model edge > 2%)
        market_prob = 1 / odds if not np.isnan(odds) else 0.33
        model_prob = probs[pred]
        edge = model_prob - market_prob
        
        if edge > 0.02:  # 2% edge threshold
            results['value_betting']['bets'] += 1
            if actual == pred:
                results['value_betting']['profit'] += (odds - 1) * stake
            else:
                results['value_betting']['profit'] -= stake
                
    # Calculate ROI
    for strategy in results:
        bets = results[strategy]['bets']
        if bets > 0:
            results[strategy]['roi'] = results[strategy]['profit'] / (bets * stake) * 100
        else:
            results[strategy]['roi'] = 0
            
    logger.info(f"  Flat Betting: {results['flat_betting']['bets']} bets, "
                f"ROI={results['flat_betting']['roi']:.2f}%")
    logger.info(f"  Value Betting: {results['value_betting']['bets']} bets, "
                f"ROI={results['value_betting']['roi']:.2f}%")
    
    return results


async def train_production_model():
    """Full training pipeline"""
    
    # Import data pipeline
    from enhanced_data_pipeline import EnhancedDataPipeline, PROCESSED_DIR
    
    # Check for existing data
    data_path = PROCESSED_DIR / "enhanced_training_data.csv"
    
    if not data_path.exists():
        logger.info("ðŸ“¥ Downloading and processing data...")
        async with EnhancedDataPipeline() as pipeline:
            df = await pipeline.download_all_historical_data()
            df = pipeline.engineer_ml_features(df)
            X, y, feature_names = pipeline.prepare_training_data(df)
            
            # Save
            training_df = pd.concat([X, y.rename("target")], axis=1)
            training_df.to_csv(data_path, index=False)
    else:
        logger.info(f"ðŸ“‚ Loading existing data from {data_path}")
        training_df = pd.read_csv(data_path)
        y = training_df['target']
        X = training_df.drop(columns=['target'])
        
    logger.info(f"ðŸ“Š Dataset: {len(X)} samples, {len(X.columns)} features")
    
    # Create and train model
    model = ProductionMLModel(model_name="sabiscore_v2")
    metrics = model.train(X, y, validate=True)
    
    # Save model
    model.save()
    
    logger.info("\n" + "="*60)
    logger.info("TRAINING COMPLETE")
    logger.info("="*60)
    logger.info(f"CV Accuracy: {metrics['cv_accuracy_mean']:.4f} (+/- {metrics['cv_accuracy_std']:.4f})")
    logger.info(f"Model saved to: {MODEL_DIR}")
    
    return model


if __name__ == "__main__":
    import asyncio
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    asyncio.run(train_production_model())
